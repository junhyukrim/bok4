{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스기사 크롤링\n",
    "# 제목 링크 날짜 퍼블리션\n",
    "# 동적 크롤링, 셀리니움(느리고 방대한데이터에 부적합, 봇으로판단돼서 차단당할수있음_)\n",
    "# 주의 해야할점\n",
    "# 1. 너무 많은 요청 보내지않기 \n",
    "# 2. 사람인것처럼 요청 보내기 User-Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://search.naver.com/search.naver?where=news&query=기준금리&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds=2008.01.02&de=2008.01.02&docid=&related=0&mynews=1&office_type=3&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom20080102to20080102&is_sug_officeid=0&office_category=3&service_area=0&start=1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200, 220805)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# 2008.01.02&de=2008.01.02 여기부분이 날짜 변경 부분\n",
    "# 그러면 여기에 날짜를 정해줄수있는 방법이있어야하나?\n",
    "# 일단 넣어보자\n",
    "find = \"기준금리\"\n",
    "start_date = \"2008.01.02\"\n",
    "end_date = \"2008.01.02\"\n",
    "page =1 \n",
    "url = f\"https://search.naver.com/search.naver?where=news&query={find}&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds={start_date}&de={end_date}&docid=&related=0&mynews=1&office_type=3&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom{start_date.replace('.', '')}to{end_date.replace('.', '')}&is_sug_officeid=0&office_category=3&service_area=0&start={page}\"\n",
    "\n",
    "print(url)\n",
    "res = requests.get(url, headers = {'User-Agent' : \"mozilla 5.0\"})\n",
    "res.status_code, res.text.find('큰기쁨예금')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째 페이지 크롤링 완료, 기사 개수: 10\n",
      "11번째 페이지 크롤링 완료, 기사 개수: 10\n",
      "21번째 페이지 크롤링 완료, 기사 개수: 4\n",
      "'2008.01.02.','파이낸셜뉴스','외환銀 ‘큰기쁨예금’ 우대금리 제공', 'http://www.fnnews.com/view?ra=Sent0701m_01A&corp=fnnews&arcid=080101220028&cDateYear=2008&cDateMonth=01&cDateDay=01&'\n",
      "'2008.01.02.','한국경제','금리 연초부터 급등', 'http://www.hankyung.com/news/app/newsview.php?aid=2008010219021'\n",
      "'2008.01.02.','이데일리','CD금리 상승세 지속..5.8%대 중반도 임박', 'http://www.edaily.co.kr/news/newspath.asp?newsid=02210726586273784'\n",
      "'2008.01.02.','매일경제','금리 껑충 … 국고채 0.12%P올라', 'http://news.mk.co.kr/news_forward.php?no=3138&year=2008'\n",
      "'2008.01.02.','한국경제','정기예금 금리 두달새 1%P 상승‥ 3개월만기 최고 5.9%', 'http://www.hankyung.com/news/app/newsview.php?aid=2008010211601'\n",
      "'2008.01.02.','이데일리','씨티은행 주택담보대출 금리 인상', 'http://www.edaily.co.kr/news/newspath.asp?newsid=01682646586273784'\n",
      "'2008.01.02.','이데일리','CD금리 상승 지속..6년7개월 최고행진', 'http://www.edaily.co.kr/news/newspath.asp?newsid=01804006586273784'\n",
      "'2008.01.02.','아시아경제','亞 증시, 올해도 '자신만만'', 'http://www.asiae.co.kr/uhtml/read.jsp?idxno=234164§ion=S1N6§ion2=S2N237'\n",
      "'2008.01.02.','파이낸셜뉴스','“弱달러 당분간 지속” 블룸버그', 'http://www.fnnews.com/view?ra=Sent1101m_01A&corp=fnnews&arcid=0921194715&cDateYear=2008&cDateMonth=01&cDateDay=02&'\n",
      "'2008.01.02.','머니투데이','가계부채 부담 내수회복세 꺾일까', 'http://www.moneytoday.co.kr/view/mtview.php?type=1&no=2007122717360177729&outlink=1'\n",
      "'2008.01.02.','이데일리','올해 美증시 둘러싼 상황 `힘들다`-CNN머니', 'http://www.edaily.co.kr/news/newspath.asp?newsid=01751526586273784'\n",
      "'2008.01.02.','파이낸셜뉴스','美 다우지수 2007년 6.4％ 상승', 'http://www.fnnews.com/view?ra=Sent0701m_01A&corp=fnnews&arcid=080101220351&cDateYear=2008&cDateMonth=01&cDateDay=01&'\n",
      "'2008.01.02.','머니투데이','가계부채 부담 내수회복세 꺾일까', 'http://www.moneytoday.co.kr/view/mtview.php?type=1&no=2007122717360177729&outlink=1'\n",
      "'2008.01.02.','아시아경제','\"달러 가치 더 떨어진다\"', 'http://www.asiae.co.kr/uhtml/read.jsp?idxno=233967§ion=S1N6§ion2='\n",
      "'2008.01.02.','이데일리','달러 낙폭, 美-세계경제 `디커플링`에 달렸다-FT', 'http://www.edaily.co.kr/news/newspath.asp?newsid=01964726586273784'\n",
      "'2008.01.02.','매일경제','美 다우지수 10% 올라 1만5000선 돌파 가능성', 'http://news.mk.co.kr/news_forward.php?no=1292&year=2008'\n",
      "'2008.01.02.','매일경제','송영규 중진공 이사 \"잘 나갈 때 사업전환 생각해야\"', 'http://news.mk.co.kr/news_forward.php?no=3090&year=2008'\n",
      "'2008.01.02.','이데일리','(투자의맥)대신증권의 \"1월효과 기대주\" 11選', 'http://www.edaily.co.kr/news/newspath.asp?newsid=01315286586273784'\n",
      "'2008.01.02.','한국경제','[2008 금융 대전망] 카드사, 쥐어짜기 경영으로 수익성 확보 총력전', 'http://www.hankyung.com/news/app/newsview.php?aid=2008010186571'\n",
      "'2008.01.02.','한국경제','<표>2일 오전 채권수익률', 'http://www.hankyung.com/news/app/newsview.php?aid=2008010207066'\n",
      "'2008.01.02.','한국경제','[2008 금융 대전망] 카드사, 쥐어짜기 경영으로 수익성 확보 총력전', 'http://www.hankyung.com/news/app/newsview.php?aid=2008010186571'\n",
      "'2008.01.02.','한국경제','<표>2일 오전 채권수익률', 'http://www.hankyung.com/news/app/newsview.php?aid=2008010207066'\n",
      "'2008.01.02.','서울경제','올 뉴욕증시 변동성 커진다 \"亞투자자 안전벨트 꽉 매라\"', 'https://n.news.naver.com/mnews/article/011/0000213857?sid=101'\n",
      "'2008.01.02.','머니투데이','2008년 안전벨트를 꽉 매라-CNN머니', 'http://www.moneytoday.co.kr/view/mtview.php?type=1&no=2008010211381051467&outlink=1'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "find = \"기준금리\"\n",
    "start_date = \"2008.01.02\"\n",
    "end_date = \"2008.01.02\"\n",
    "page = 1  \n",
    "articles = []\n",
    "\n",
    "while True:\n",
    "    url = f\"https://search.naver.com/search.naver?where=news&query={find}&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds={start_date}&de={end_date}&docid=&related=0&mynews=1&office_type=3&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom{start_date.replace('.', '')}to{end_date.replace('.', '')}&is_sug_officeid=0&office_category=3&service_area=0&start={page}\"\n",
    "    # f스트링으로 써야 저장한 변수가 적용됨\n",
    "    # User-Agent 추가 (네이버에서 봇으로 차단되지 않도록)\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36\"}\n",
    "    \n",
    "    # 페이지 요청\n",
    "    res = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    \n",
    "    # 기사 목록 가져오기\n",
    "    news_list = soup.select(\".news_area\")  \n",
    "    # .bx 너무 많은 부분이 떠버림\n",
    "    \n",
    "    # 더 이상 기사가 없으면 종료\n",
    "    if not news_list:\n",
    "        break\n",
    "\n",
    "    for news in news_list:\n",
    "        title_tag = news.select_one(\".news_tit\")  # 제목 태그\n",
    "        press_tag = news.select_one(\".info.press\")  # 신문사 태그\n",
    "        date_tag = news.select_one(\"span.info\")  # 날짜 태그 \n",
    "        \n",
    "        if title_tag and press_tag and date_tag:\n",
    "            title = title_tag.text.strip()  # 기사 제목\n",
    "            link = title_tag[\"href\"]  # 기사 링크\n",
    "            news_agency = press_tag.text.strip()  # 신문사 이름\n",
    "            article_date = date_tag.text.strip()  # 날짜\n",
    "            \n",
    "            articles.append((title, news_agency, article_date, link))\n",
    "    \n",
    "    print(f\"{page}번째 페이지 크롤링 완료, 기사 개수: {len(news_list)}\")\n",
    "    \n",
    "    # 다음 페이지로 이동\n",
    "    page += 10  # 10개씩 넘어감 \n",
    "    # 5개로 줄여봤는데 중복으로 기사를 가져옴\n",
    "\n",
    "    import time\n",
    "    import random\n",
    "    # 차단 방지 1~2초 쉬어 주기\n",
    "    sleep_time = random.uniform(1,2) # 몰라서 물어봄...\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "for idx,(title, news_agency, article_date, link) in enumerate(articles, 1):\n",
    "    print(f\"'{article_date}','{news_agency}','{title}', '{link}'\") \n",
    "    # csv 로 저장하려고 , 로 넣었는데 이거 기사제목에도 , 이게 붙지않나?\n",
    "    # 그러면 각각 문자열로 바꿔주면 기사 제목에 , 도 csv 에선 문자열로 인식\n",
    "    # 이제 이걸 csv로 저장\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
