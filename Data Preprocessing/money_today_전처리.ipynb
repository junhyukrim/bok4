{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\wosle\\anaconda3\\envs\\crawl\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\wosle\\anaconda3\\envs\\crawl\\lib\\site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\wosle\\anaconda3\\envs\\crawl\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\wosle\\anaconda3\\envs\\crawl\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\wosle\\anaconda3\\envs\\crawl\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\wosle\\anaconda3\\envs\\crawl\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\wosle\\anaconda3\\envs\\crawl\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\wosle\\anaconda3\\envs\\crawl\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\wosle\\anaconda3\\envs\\crawl\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 40.1 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp39-cp39-win_amd64.whl (274 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, nltk\n",
      "Successfully installed nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wosle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wosle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date                                       doc_id  sntc_id  \\\n",
      "0        date                      news_date_moneytoday_id        1   \n",
      "1  2008-01-01  news_20080101_moneytoday_ '남미병' 털고 조용한 혁명중\"        1   \n",
      "2  2008-01-01  news_20080101_moneytoday_ '남미병' 털고 조용한 혁명중\"        2   \n",
      "3  2008-01-01  news_20080101_moneytoday_ '남미병' 털고 조용한 혁명중\"        3   \n",
      "4  2008-01-01  news_20080101_moneytoday_ '남미병' 털고 조용한 혁명중\"        4   \n",
      "\n",
      "                                           orgn_sntc  \n",
      "0                                            content  \n",
      "1  0000870251 , 브라질 경제12 최경하 한국수출입은행 상파울루 사무소 소장은...  \n",
      "2  최근 헤알이 급격히 절상되면서 브라질이 수출 경쟁력을 잃어 자동차 수출이 줄고 있다...  \n",
      "3     브라질 내수 경기가 호황이란 사실은 LG전자의 판매 추이에서도 확인할 수 있었다 .  \n",
      "4  박성학 LG전자 브라질 법인 부장은 매출이 2006년에 46 늘어난데 이어 2007...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# NLTK에서 불용어(stopwords)와 토큰화 기능 다운로드\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 불용어 목록 로드\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 데이터 파일 경로 지정\n",
    "input_file_path = 'C:/Users/wosle/OneDrive/Desktop/Bok_Projet_woslek/뉴스 클랜징/머니투데이/머니투데이클렌징7.csv'  # 입력 데이터 파일 경로\n",
    "output_file_path = 'C:/Users/wosle/OneDrive/Desktop/Bok_Projet_woslek/뉴스 클랜징/머니투데이/머니투데이전처리2.csv'  # 출력 데이터 파일 경로\n",
    "\n",
    "# 파일을 읽어서 텍스트로 변환하는 함수\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "    return data\n",
    "\n",
    "# 문장을 구두점 기준으로 나누는 함수\n",
    "def split_sentences(text):\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<=\\.|\\?|\\!)\\s', text)\n",
    "    return sentences\n",
    "\n",
    "# 불용어 처리 함수\n",
    "def remove_stopwords(sentence):\n",
    "    # \"기자\"라는 단어가 포함된 문장은 제외\n",
    "    if \"기자\" in sentence:\n",
    "        return None\n",
    "    if \"네고가능\" in sentence:\n",
    "        return None\n",
    "    if \"무단 및 중\" in sentence:\n",
    "        return None\n",
    "    if \"R .\" in sentence:\n",
    "        return None\n",
    "    \n",
    "    words = word_tokenize(sentence)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# 문장 데이터를 구두점 기준으로 나누고, 불용어 처리 후 CSV 형식으로 변환\n",
    "def process_text(data):\n",
    "    lines = data.splitlines()\n",
    "    processed_data = []\n",
    "\n",
    "    for line in lines:\n",
    "        date, title, doc_id, content = line.split(\",\", 3)\n",
    "        sentences = split_sentences(content)\n",
    "        \n",
    "        # 문장 나누기 및 불용어 처리\n",
    "        for idx, sentence in enumerate(sentences):\n",
    "            cleaned_sentence = remove_stopwords(sentence)\n",
    "            if cleaned_sentence:  # \"기자\" 단어가 포함된 문장은 None이므로 제외\n",
    "                processed_data.append([date, f\"news_{date.replace('-', '')}_moneytoday_{doc_id}\", idx+1, cleaned_sentence])\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# 파일에서 데이터를 읽어와 처리하고 CSV로 저장\n",
    "data = read_file(input_file_path)  # 텍스트 데이터 읽기\n",
    "processed_data = process_text(data)  # 텍스트 처리\n",
    "\n",
    "# DataFrame으로 변환 후 CSV로 저장\n",
    "df = pd.DataFrame(processed_data, columns=[\"date\", \"doc_id\", \"sntc_id\", \"orgn_sntc\"])\n",
    "df.to_csv(output_file_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(df.head())  # 처리된 데이터 확인\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
