{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Downloading Scrapy-2.12.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting Twisted>=21.7.0 (from scrapy)\n",
      "  Downloading twisted-24.11.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting cryptography>=37.0.0 (from scrapy)\n",
      "  Downloading cryptography-44.0.1-cp39-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting cssselect>=0.9.1 (from scrapy)\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting itemloaders>=1.0.1 (from scrapy)\n",
      "  Downloading itemloaders-1.3.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting parsel>=1.5.0 (from scrapy)\n",
      "  Downloading parsel-1.10.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pyOpenSSL>=22.0.0 (from scrapy)\n",
      "  Using cached pyOpenSSL-25.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting queuelib>=1.4.2 (from scrapy)\n",
      "  Downloading queuelib-1.7.0-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting service-identity>=18.1.0 (from scrapy)\n",
      "  Downloading service_identity-24.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting w3lib>=1.17.0 (from scrapy)\n",
      "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting zope.interface>=5.1.0 (from scrapy)\n",
      "  Downloading zope.interface-7.2-cp39-cp39-win_amd64.whl.metadata (45 kB)\n",
      "Collecting protego>=0.1.15 (from scrapy)\n",
      "  Downloading Protego-0.4.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting itemadapter>=0.1.0 (from scrapy)\n",
      "  Downloading itemadapter-0.11.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\wosle\\anaconda3\\envs\\crawl\\lib\\site-packages (from scrapy) (24.2)\n",
      "Collecting tldextract (from scrapy)\n",
      "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting lxml>=4.6.0 (from scrapy)\n",
      "  Downloading lxml-5.3.1-cp39-cp39-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting defusedxml>=0.7.1 (from scrapy)\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
      "  Downloading PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\wosle\\anaconda3\\envs\\crawl\\lib\\site-packages (from cryptography>=37.0.0->scrapy) (1.17.1)\n",
      "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.9 in c:\\users\\wosle\\anaconda3\\envs\\crawl\\lib\\site-packages (from pyOpenSSL>=22.0.0->scrapy) (4.12.2)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\wosle\\anaconda3\\envs\\crawl\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (24.3.0)\n",
      "Collecting pyasn1 (from service-identity>=18.1.0->scrapy)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pyasn1-modules (from service-identity>=18.1.0->scrapy)\n",
      "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting automat>=24.8.0 (from Twisted>=21.7.0->scrapy)\n",
      "  Downloading Automat-24.8.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting constantly>=15.1 (from Twisted>=21.7.0->scrapy)\n",
      "  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting hyperlink>=17.1.1 (from Twisted>=21.7.0->scrapy)\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting incremental>=24.7.0 (from Twisted>=21.7.0->scrapy)\n",
      "  Downloading incremental-24.7.2-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wosle\\anaconda3\\envs\\crawl\\lib\\site-packages (from zope.interface>=5.1.0->scrapy) (75.1.0)\n",
      "Requirement already satisfied: idna in c:\\users\\wosle\\anaconda3\\envs\\crawl\\lib\\site-packages (from tldextract->scrapy) (3.10)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\wosle\\anaconda3\\envs\\crawl\\lib\\site-packages (from tldextract->scrapy) (2.32.3)\n",
      "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
      "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting filelock>=3.0.8 (from tldextract->scrapy)\n",
      "  Downloading filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\wosle\\anaconda3\\envs\\crawl\\lib\\site-packages (from cffi>=1.12->cryptography>=37.0.0->scrapy) (2.22)\n",
      "Collecting tomli (from incremental>=24.7.0->Twisted>=21.7.0->scrapy)\n",
      "  Downloading tomli-2.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\wosle\\anaconda3\\envs\\crawl\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\wosle\\anaconda3\\envs\\crawl\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wosle\\anaconda3\\envs\\crawl\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2024.12.14)\n",
      "Downloading Scrapy-2.12.0-py2.py3-none-any.whl (311 kB)\n",
      "Downloading cryptography-44.0.1-cp39-abi3-win_amd64.whl (3.2 MB)\n",
      "   ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "   ----------------------------- ---------- 2.4/3.2 MB 12.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.2/3.2 MB 11.1 MB/s eta 0:00:00\n",
      "Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Downloading itemadapter-0.11.0-py3-none-any.whl (11 kB)\n",
      "Downloading itemloaders-1.3.2-py3-none-any.whl (12 kB)\n",
      "Downloading lxml-5.3.1-cp39-cp39-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 2.4/3.8 MB 10.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 10.8 MB/s eta 0:00:00\n",
      "Downloading parsel-1.10.0-py2.py3-none-any.whl (17 kB)\n",
      "Downloading Protego-0.4.0-py2.py3-none-any.whl (8.6 kB)\n",
      "Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
      "Using cached pyOpenSSL-25.0.0-py3-none-any.whl (56 kB)\n",
      "Downloading queuelib-1.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Downloading service_identity-24.2.0-py3-none-any.whl (11 kB)\n",
      "Downloading twisted-24.11.0-py3-none-any.whl (3.2 MB)\n",
      "   ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 1.0/3.2 MB 7.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.2/3.2 MB 9.9 MB/s eta 0:00:00\n",
      "Downloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
      "Downloading zope.interface-7.2-cp39-cp39-win_amd64.whl (211 kB)\n",
      "Downloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
      "Downloading Automat-24.8.1-py3-none-any.whl (42 kB)\n",
      "Downloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
      "Downloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "Downloading incremental-24.7.2-py3-none-any.whl (20 kB)\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Downloading tomli-2.2.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: PyDispatcher, zope.interface, w3lib, tomli, queuelib, pyasn1, protego, lxml, jmespath, itemadapter, hyperlink, filelock, defusedxml, cssselect, constantly, automat, requests-file, pyasn1-modules, parsel, incremental, cryptography, Twisted, tldextract, service-identity, pyOpenSSL, itemloaders, scrapy\n",
      "Successfully installed PyDispatcher-2.0.7 Twisted-24.11.0 automat-24.8.1 constantly-23.10.4 cryptography-44.0.1 cssselect-1.2.0 defusedxml-0.7.1 filelock-3.17.0 hyperlink-21.0.0 incremental-24.7.2 itemadapter-0.11.0 itemloaders-1.3.2 jmespath-1.0.1 lxml-5.3.1 parsel-1.10.0 protego-0.4.0 pyOpenSSL-25.0.0 pyasn1-0.6.1 pyasn1-modules-0.4.1 queuelib-1.7.0 requests-file-2.1.0 scrapy-2.12.0 service-identity-24.2.0 tldextract-5.1.3 tomli-2.2.1 w3lib-2.3.1 zope.interface-7.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import scrapy\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "class NewsSpider(scrapy.Spider):\n",
    "    name = \"news\"\n",
    "    custom_settings = {\n",
    "        'JOBDIR': 'crawls/news_spider_state',\n",
    "        # 'FEED': {} # Scrapy 기본 저장 방식 비활성화\n",
    "    }\n",
    "\n",
    "    # 합친 csv에서 url 목록 불러오기\n",
    "    def start_requests(self):\n",
    "        df = pd.read_csv('C:/Users/wosle/OneDrive/Desktop/Bok_Projet_woslek/Financial_News_text')\n",
    "        urls = df['url'].tolist()\n",
    "\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    # 웹에서 본문 크롤링\n",
    "    def parse(self, response):\n",
    "        title = response.css('h2#title_area span::text').get(default='N/A')\n",
    "        date_raw = response.css('span.media_end_head_info_datestamp_time._ARTICLE_DATE_TIME::attr(data-date-time)').get(default=\"N/A\")\n",
    "        content = ' '.join(response.css('div.newsct_article p::text').getall()).strip()\n",
    "\n",
    "        # 로그에 date_raw 값 출력 (디버깅용)\n",
    "        self.log(f\"DEBUG: Extracted date_raw: {date_raw}\")\n",
    "\n",
    "        # 날짜에서 시간 제거\n",
    "        if date_raw and date_raw != 'N/A':\n",
    "            clean_date = date_raw.split(' ')[0]\n",
    "        else:\n",
    "            clean_date = 'unknown_date'\n",
    "\n",
    "        # URL에서 고유 기사 ID 추출\n",
    "        article_id_match = re.search(r'/(\\d{10})\\?', response.url)\n",
    "        article_id = article_id_match.group(1) if article_id_match else \"no_id\"\n",
    "\n",
    "        # 텍스트파일 저장할 폴더 생성 (없으면 자동 생성)\n",
    "        save_path = \"news_texts\"\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        \n",
    "        # 파일명 만들기\n",
    "        file_title = re.sub(r'[\\\\/*?:\"<>|]', \"_\", clean_date)\n",
    "        file_name = f'news_{file_title}_{article_id}.txt'\n",
    "\n",
    "        with open(f'news_texts/{file_name}','w', encoding='utf-8') as f:\n",
    "            f.write(f'{title}\\n')\n",
    "            f.write(f'{clean_date}\\n')\n",
    "            f.write(f'{content}\\n')\n",
    "\n",
    "        self.log(f'Saved file: {file_name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
